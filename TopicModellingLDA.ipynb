{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TopicModellingLDA.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Czadq7cpOssj",
        "colab_type": "text"
      },
      "source": [
        "LDA is a matrix factorization technique. In vector space, any corpus (collection of documents) can be represented as a document-term matrix. The following matrix shows a corpus of N documents D1, D2, D3 … Dn and vocabulary size of M words W1,W2 .. Wn. The value of i,j cell gives the frequency count of word Wj in Document Di.\n",
        "\n",
        "LDA converts this Document-Term Matrix into two lower dimensional matrices – M1 and M2.\n",
        "M1 is a document-topics matrix and M2 is a topic – terms matrix with dimensions (N,  K) and (K, M) respectively, where N is the number of documents, K is the number of topics and M is the vocabulary size.\n",
        "\n",
        "Alpha and Beta Hyperparameters – alpha represents document-topic density and Beta represents topic-word density. Higher the value of alpha, documents are composed of more topics and lower the value of alpha, documents contain fewer topics. On the other hand, higher the beta, topics are composed of a large number of words in the corpus, and with the lower value of beta, they are composed of few words.\n",
        "\n",
        "All the text documents combined is known as the corpus. To run any mathematical model on text corpus, it is a good practice to convert it into a matrix representation. LDA model looks for repeating term patterns in the entire DT matrix. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b0RR0pT7PLgP",
        "colab_type": "code",
        "outputId": "831360a1-14d4-4192-9b6f-690d758e186d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        }
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.read_json('https://raw.githubusercontent.com/selva86/datasets/master/newsgroups.json')\n",
        "#data_text = data[['headline_text']]\n",
        "#data_text['index'] = data_text.index\n",
        "#documents = data_text\n",
        "df.head(10)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>content</th>\n",
              "      <th>target</th>\n",
              "      <th>target_names</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>From: lerxst@wam.umd.edu (where's my thing)\\nS...</td>\n",
              "      <td>7</td>\n",
              "      <td>rec.autos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>From: guykuo@carson.u.washington.edu (Guy Kuo)...</td>\n",
              "      <td>4</td>\n",
              "      <td>comp.sys.mac.hardware</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>From: irwin@cmptrc.lonestar.org (Irwin Arnstei...</td>\n",
              "      <td>8</td>\n",
              "      <td>rec.motorcycles</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>100</th>\n",
              "      <td>From: tchen@magnus.acs.ohio-state.edu (Tsung-K...</td>\n",
              "      <td>6</td>\n",
              "      <td>misc.forsale</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1000</th>\n",
              "      <td>From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...</td>\n",
              "      <td>2</td>\n",
              "      <td>comp.os.ms-windows.misc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10000</th>\n",
              "      <td>From: a207706@moe.dseg.ti.com (Robert Loper)\\n...</td>\n",
              "      <td>7</td>\n",
              "      <td>rec.autos</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10001</th>\n",
              "      <td>From: kimman@magnus.acs.ohio-state.edu (Kim Ri...</td>\n",
              "      <td>6</td>\n",
              "      <td>misc.forsale</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10002</th>\n",
              "      <td>From: kwilson@casbah.acns.nwu.edu (Kirtley Wil...</td>\n",
              "      <td>2</td>\n",
              "      <td>comp.os.ms-windows.misc</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10003</th>\n",
              "      <td>Subject: Re: Don't more innocents die without ...</td>\n",
              "      <td>0</td>\n",
              "      <td>alt.atheism</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10004</th>\n",
              "      <td>From: livesey@solntze.wpd.sgi.com (Jon Livesey...</td>\n",
              "      <td>0</td>\n",
              "      <td>alt.atheism</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                 content  ...             target_names\n",
              "0      From: lerxst@wam.umd.edu (where's my thing)\\nS...  ...                rec.autos\n",
              "1      From: guykuo@carson.u.washington.edu (Guy Kuo)...  ...    comp.sys.mac.hardware\n",
              "10     From: irwin@cmptrc.lonestar.org (Irwin Arnstei...  ...          rec.motorcycles\n",
              "100    From: tchen@magnus.acs.ohio-state.edu (Tsung-K...  ...             misc.forsale\n",
              "1000   From: dabl2@nlm.nih.gov (Don A.B. Lindbergh)\\n...  ...  comp.os.ms-windows.misc\n",
              "10000  From: a207706@moe.dseg.ti.com (Robert Loper)\\n...  ...                rec.autos\n",
              "10001  From: kimman@magnus.acs.ohio-state.edu (Kim Ri...  ...             misc.forsale\n",
              "10002  From: kwilson@casbah.acns.nwu.edu (Kirtley Wil...  ...  comp.os.ms-windows.misc\n",
              "10003  Subject: Re: Don't more innocents die without ...  ...              alt.atheism\n",
              "10004  From: livesey@solntze.wpd.sgi.com (Jon Livesey...  ...              alt.atheism\n",
              "\n",
              "[10 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8veh_IZ-dOy",
        "colab_type": "code",
        "outputId": "50df1382-495a-4f39-fbee-e2af78f57a11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        }
      },
      "source": [
        "data_text = df[['content']]\n",
        "data_text['index'] = data_text.index\n",
        "documents = data_text\n",
        "documents.shape"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(11314, 2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXqjJjWF-EKk",
        "colab_type": "code",
        "outputId": "d109dd26-b92d-429a-80dc-b6cd24fcbf53",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        }
      },
      "source": [
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "from gensim.parsing.preprocessing import STOPWORDS\n",
        "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
        "from nltk.stem.porter import *\n",
        "from nltk.stem import PorterStemmer\n",
        "import numpy as np\n",
        "np.random.seed(2018)\n",
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d04WsISv09_6",
        "colab_type": "text"
      },
      "source": [
        "Tokenization: Split the text into sentences and the sentences into words. \n",
        "Lowercase the words and remove punctuation.\n",
        "Words that have fewer than 3 characters are removed.\n",
        "All stopwords are removed.\n",
        "Words are lemmatized — words in third person are changed to first person and verbs in past and future tenses are changed into present.\n",
        "Words are stemmed — words are reduced to their root form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEt3ardLgcOJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "porter = PorterStemmer()\n",
        "def lemmatize_stemming(text):\n",
        "    return porter.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
        "  \n",
        "def preprocess(text):\n",
        "    result = []\n",
        "    for token in gensim.utils.simple_preprocess(text):\n",
        "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
        "            result.append(lemmatize_stemming(token))\n",
        "    return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qwUgxBki6bkk",
        "colab_type": "text"
      },
      "source": [
        "PorterStemmer uses Suffix Stripping to produce stems. Notice how the PorterStemmer is giving the root (stem) of the word \"cats\" by simply removing the 's' after cat. This is a suffix added to cat to make it plural.\n",
        "PorterStemmer algorithm does not follow linguistics rather a set of 05 rules for different cases that are applied in phases (step by step) to generate stems\n",
        "his is the reason why PorterStemmer does not often generate stems that are actual English words. It does not keep a lookup table for actual stems of the word but applies algorithmic rules to generate stems. It uses the rules to decide whether it is wise to strip a suffix.\n",
        "\n",
        "SnowballStemmers is used to create non-English Stemmers!\n",
        "\n",
        "The LancasterStemmer (Paice-Husk stemmer) is an iterative algorithm with rules saved externally. One table containing about 120 rules indexed by the last letter of a suffix. On each iteration, it tries to find an applicable rule by the last character of the word. Each rule specifies either a deletion or replacement of an ending. If there is no such rule, it terminates. It also terminates if a word starts with a vowel and there are only two letters left or if a word starts with a consonant and there are only three characters left. Otherwise, the rule is applied, and the process repeats. LancasterStemmer is simple, but heavy stemming due to iterations and over-stemming may occur. Over-stemming causes the stems to be not linguistic, or they may have no meaning"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQZCXIAKgiD6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "8bec3f37-4f7e-4a1c-bfb9-2cc6a588d104"
      },
      "source": [
        "doc_sample = documents[documents['index'] == 0].values[0][0]\n",
        "print('original document: ')\n",
        "words = []\n",
        "\n",
        "for word in doc_sample.split(' '):\n",
        "    words.append(word)\n",
        "print(words)\n",
        "print('\\n\\n tokenized and lemmatized document: ')\n",
        "print(preprocess(doc_sample))"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "original document: \n",
            "['From:', 'lerxst@wam.umd.edu', \"(where's\", 'my', 'thing)\\nSubject:', 'WHAT', 'car', 'is', 'this!?\\nNntp-Posting-Host:', 'rac3.wam.umd.edu\\nOrganization:', 'University', 'of', 'Maryland,', 'College', 'Park\\nLines:', '15\\n\\n', 'I', 'was', 'wondering', 'if', 'anyone', 'out', 'there', 'could', 'enlighten', 'me', 'on', 'this', 'car', 'I', 'saw\\nthe', 'other', 'day.', 'It', 'was', 'a', '2-door', 'sports', 'car,', 'looked', 'to', 'be', 'from', 'the', 'late', '60s/\\nearly', '70s.', 'It', 'was', 'called', 'a', 'Bricklin.', 'The', 'doors', 'were', 'really', 'small.', 'In', 'addition,\\nthe', 'front', 'bumper', 'was', 'separate', 'from', 'the', 'rest', 'of', 'the', 'body.', 'This', 'is', '\\nall', 'I', 'know.', 'If', 'anyone', 'can', 'tellme', 'a', 'model', 'name,', 'engine', 'specs,', 'years\\nof', 'production,', 'where', 'this', 'car', 'is', 'made,', 'history,', 'or', 'whatever', 'info', 'you\\nhave', 'on', 'this', 'funky', 'looking', 'car,', 'please', 'e-mail.\\n\\nThanks,\\n-', 'IL\\n', '', '', '----', 'brought', 'to', 'you', 'by', 'your', 'neighborhood', 'Lerxst', '----\\n\\n\\n\\n\\n']\n",
            "\n",
            "\n",
            " tokenized and lemmatized document: \n",
            "['lerxst', 'thing', 'subject', 'nntp', 'post', 'host', 'organ', 'univers', 'maryland', 'colleg', 'park', 'line', 'wonder', 'enlighten', 'door', 'sport', 'look', 'late', 'earli', 'call', 'bricklin', 'door', 'small', 'addit', 'bumper', 'separ', 'rest', 'bodi', 'know', 'tellm', 'model', 'engin', 'spec', 'year', 'product', 'histori', 'info', 'funki', 'look', 'mail', 'thank', 'bring', 'neighborhood', 'lerxst']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7IssciG7A2U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "138c714d-23f4-4671-f654-1a97bb23ecd9"
      },
      "source": [
        "processed_docs = documents['content'].map(preprocess)\n",
        "processed_docs[:10]"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        [lerxst, thing, subject, nntp, post, host, org...\n",
              "1        [guykuo, carson, washington, subject, clock, p...\n",
              "10       [irwin, cmptrc, lonestar, irwin, arnstein, sub...\n",
              "100      [tchen, magnu, ohio, state, tsung, chen, subje...\n",
              "1000     [dabl, lindbergh, subject, diamond, mous, curs...\n",
              "10000    [dseg, robert, loper, subject, nntp, post, hos...\n",
              "10001    [kimman, magnu, ohio, state, richard, subject,...\n",
              "10002    [kwilson, casbah, acn, kirtley, wilson, subjec...\n",
              "10003    [subject, innoc, death, penalti, bobb, vice, r...\n",
              "10004    [livesey, solntz, livesey, subject, genocid, c...\n",
              "Name: content, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhXqJZY87OCa",
        "colab_type": "text"
      },
      "source": [
        "corpora.dictionary – Construct word<->id mappings \n",
        "This module implements the concept of a Dictionary – a mapping between words and their integer ids."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahiUwe6i7OPs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 210
        },
        "outputId": "44336c81-21a1-4c0c-84b8-4ce6f98f9780"
      },
      "source": [
        "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
        "count = 0\n",
        "for k, v in dictionary.iteritems():\n",
        "    print(k, v)\n",
        "    count += 1\n",
        "    if count > 10:\n",
        "        break"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 addit\n",
            "1 bodi\n",
            "2 bricklin\n",
            "3 bring\n",
            "4 bumper\n",
            "5 call\n",
            "6 colleg\n",
            "7 door\n",
            "8 earli\n",
            "9 engin\n",
            "10 enlighten\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKYrV6xB8J3H",
        "colab_type": "text"
      },
      "source": [
        "Filter out tokens that appear in\n",
        "less than 15 documents (absolute number) or\n",
        "more than 0.5 documents (fraction of total corpus size, not absolute number).\n",
        "after the above two steps, keep only the first 100000 most frequent tokens."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kd-_l7uu8MQG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYp_knda8nG4",
        "colab_type": "text"
      },
      "source": [
        "doc2bow(document, allow_update=False, return_missing=False)\n",
        "Convert document into the bag-of-words (BoW) format = list of (token_id, token_count) tuples.\n",
        "\n",
        "\n",
        "Parameters:\n",
        "document (list of str) – Input document.\n",
        "\n",
        "allow_update (bool, optional) – Update self, by adding new tokens from document and updating internal corpus statistics.\n",
        "\n",
        "return_missing (bool, optional) – Return missing tokens (tokens present in document but not in self) with frequencies?\n",
        "Returns:\n",
        "\n",
        "list of (int, int) – BoW representation of document.\n",
        "\n",
        "list of (int, int), dict of (str, int) – If return_missing is True, return BoW representation of document + dictionary with missing tokens and their frequencies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_aDbP2d8v7Z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "outputId": "8c7c0bdb-50ce-4019-eb9a-2b45f14dc179"
      },
      "source": [
        "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "bow_corpus[0]"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(0, 1),\n",
              " (1, 1),\n",
              " (2, 1),\n",
              " (3, 1),\n",
              " (4, 1),\n",
              " (5, 1),\n",
              " (6, 2),\n",
              " (7, 1),\n",
              " (8, 1),\n",
              " (9, 1),\n",
              " (10, 1),\n",
              " (11, 1),\n",
              " (12, 1),\n",
              " (13, 1),\n",
              " (14, 1),\n",
              " (15, 2),\n",
              " (16, 1),\n",
              " (17, 1),\n",
              " (18, 1),\n",
              " (19, 1),\n",
              " (20, 1),\n",
              " (21, 1),\n",
              " (22, 1),\n",
              " (23, 1),\n",
              " (24, 1),\n",
              " (25, 1),\n",
              " (26, 1),\n",
              " (27, 1),\n",
              " (28, 1),\n",
              " (29, 1),\n",
              " (30, 1),\n",
              " (31, 1),\n",
              " (32, 1)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "anI7y72L852U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "outputId": "41dff9c9-729e-458a-ed52-c411a1de0b29"
      },
      "source": [
        "bow_doc_0 = bow_corpus[0]\n",
        "for i in range(len(bow_doc_0)):\n",
        "    print(\"Word {} (\\\"{}\\\") appears {} time.\".format(bow_doc_0[i][0], \n",
        "                                               dictionary[bow_doc_0[i][0]], \n",
        "bow_doc_0[i][1]))"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Word 0 (\"addit\") appears 1 time.\n",
            "Word 1 (\"bodi\") appears 1 time.\n",
            "Word 2 (\"bring\") appears 1 time.\n",
            "Word 3 (\"bumper\") appears 1 time.\n",
            "Word 4 (\"call\") appears 1 time.\n",
            "Word 5 (\"colleg\") appears 1 time.\n",
            "Word 6 (\"door\") appears 2 time.\n",
            "Word 7 (\"earli\") appears 1 time.\n",
            "Word 8 (\"engin\") appears 1 time.\n",
            "Word 9 (\"enlighten\") appears 1 time.\n",
            "Word 10 (\"histori\") appears 1 time.\n",
            "Word 11 (\"host\") appears 1 time.\n",
            "Word 12 (\"info\") appears 1 time.\n",
            "Word 13 (\"know\") appears 1 time.\n",
            "Word 14 (\"late\") appears 1 time.\n",
            "Word 15 (\"look\") appears 2 time.\n",
            "Word 16 (\"mail\") appears 1 time.\n",
            "Word 17 (\"maryland\") appears 1 time.\n",
            "Word 18 (\"model\") appears 1 time.\n",
            "Word 19 (\"neighborhood\") appears 1 time.\n",
            "Word 20 (\"nntp\") appears 1 time.\n",
            "Word 21 (\"park\") appears 1 time.\n",
            "Word 22 (\"product\") appears 1 time.\n",
            "Word 23 (\"rest\") appears 1 time.\n",
            "Word 24 (\"separ\") appears 1 time.\n",
            "Word 25 (\"small\") appears 1 time.\n",
            "Word 26 (\"spec\") appears 1 time.\n",
            "Word 27 (\"sport\") appears 1 time.\n",
            "Word 28 (\"thank\") appears 1 time.\n",
            "Word 29 (\"thing\") appears 1 time.\n",
            "Word 30 (\"univers\") appears 1 time.\n",
            "Word 31 (\"wonder\") appears 1 time.\n",
            "Word 32 (\"year\") appears 1 time.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jlQT-uIO9cgD",
        "colab_type": "text"
      },
      "source": [
        "In information retrieval, tf–idf or TFIDF, short for term frequency–inverse document frequency, is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus.It is often used as a weighting factor in searches of information retrieval, text mining, and user modeling.\n",
        "\n",
        "The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXEJj7pI94HU",
        "colab_type": "text"
      },
      "source": [
        "The pprint module provides a capability to “pretty-print” arbitrary Python data structures in a form which can be used as input to the interpreter. If the formatted structures include objects which are not fundamental Python types, the representation may not be loadable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2LRUMwP9jXZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "outputId": "dbf90881-97ff-4360-d6f3-8e7a6ebf7737"
      },
      "source": [
        "from gensim import corpora, models\n",
        "tfidf = models.TfidfModel(bow_corpus)\n",
        "corpus_tfidf = tfidf[bow_corpus]\n",
        "from pprint import pprint\n",
        "for doc in corpus_tfidf:\n",
        "    pprint(doc)\n",
        "    break"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(0, 0.16531596959995393),\n",
            " (1, 0.16785300112088367),\n",
            " (2, 0.15019839074046323),\n",
            " (3, 0.2858093866531402),\n",
            " (4, 0.11438968484597732),\n",
            " (5, 0.15167771657985907),\n",
            " (6, 0.3893228656323328),\n",
            " (7, 0.16890540828616113),\n",
            " (8, 0.12279310710364673),\n",
            " (9, 0.26345362807652206),\n",
            " (10, 0.164457883902288),\n",
            " (11, 0.041906971663713447),\n",
            " (12, 0.13943545149619074),\n",
            " (13, 0.0532662930087635),\n",
            " (14, 0.17840425276381963),\n",
            " (15, 0.16145581407375503),\n",
            " (16, 0.1018221519189483),\n",
            " (17, 0.2333916943145223),\n",
            " (18, 0.16220115600873633),\n",
            " (19, 0.2804570052453337),\n",
            " (20, 0.042646900587070304),\n",
            " (21, 0.18493988543555143),\n",
            " (22, 0.1486736252074099),\n",
            " (23, 0.15971058881319783),\n",
            " (24, 0.18156419819616518),\n",
            " (25, 0.14476899410325858),\n",
            " (26, 0.20971806193095166),\n",
            " (27, 0.19290262455075843),\n",
            " (28, 0.08432119282836148),\n",
            " (29, 0.08377121895934883),\n",
            " (30, 0.04441770413224336),\n",
            " (31, 0.13721451448079763),\n",
            " (32, 0.08432119282836148)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ULERpoVf98xk",
        "colab_type": "text"
      },
      "source": [
        "Train our lda model using gensim.models.LdaMulticore and save it to ‘lda_model’"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxspMCO_-P28",
        "colab_type": "text"
      },
      "source": [
        "models.ldamulticore – parallelized Latent Dirichlet Allocation \n",
        "Online Latent Dirichlet Allocation (LDA) in Python, using all CPU cores to parallelize and speed up model training.\n",
        "The parallelization uses multiprocessing\n",
        "\n",
        "gensim.models.ldamodel.LdaModel class is an equivalent, but more straightforward and single-core implementation.\n",
        "\n",
        "The training algorithm:\n",
        "is streamed: training documents may come in sequentially, no random access required,\n",
        "runs in constant memory w.r.t. the number of documents: size of the training corpus does not affect memory footprint, can process corpora larger than RAM\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s2HvWgwL-tHH",
        "colab_type": "text"
      },
      "source": [
        "Parameters:\n",
        "\n",
        "corpus ({iterable of list of (int, float), scipy.sparse.csc}, optional) – Stream of document vectors or sparse matrix of shape (num_terms, num_documents). If not given, the model is left untrained (presumably because you want to call update() manually).\n",
        "\n",
        "num_topics (int, optional) – The number of requested latent topics to be extracted from the training corpus.\n",
        "\n",
        "id2word ({dict of (int, str), gensim.corpora.dictionary.Dictionary}) – Mapping from word IDs to words. It is used to determine the vocabulary size, as well as for debugging and topic printing.\n",
        "\n",
        "workers (int, optional) – Number of workers processes to be used for parallelization. If None all available cores (as estimated by workers=cpu_count()-1 will be used. \n",
        "\n",
        "Note however that for hyper-threaded CPUs, this estimation returns a too high number – set workers directly to the number of your real cores (not hyperthreads) minus one, for optimal performance."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVAGT9Ot972M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lda_model = gensim.models.LdaMulticore(bow_corpus, num_topics=10, id2word=dictionary, passes=3, workers=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDVmLHrA_B_9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "outputId": "8e5056d6-5a6f-4d0b-8162-0d47c4b7deb5"
      },
      "source": [
        "for idx, topic in lda_model.print_topics(-1):\n",
        "    print('Topic: {} \\nWords: {}'.format(idx, topic))"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 0 \n",
            "Words: 0.008*\"univers\" + 0.007*\"articl\" + 0.007*\"year\" + 0.006*\"know\" + 0.006*\"host\" + 0.006*\"nntp\" + 0.005*\"team\" + 0.005*\"like\" + 0.004*\"think\" + 0.004*\"david\"\n",
            "Topic: 1 \n",
            "Words: 0.011*\"articl\" + 0.009*\"think\" + 0.007*\"like\" + 0.006*\"game\" + 0.006*\"nntp\" + 0.005*\"host\" + 0.005*\"peopl\" + 0.005*\"univers\" + 0.005*\"team\" + 0.005*\"know\"\n",
            "Topic: 2 \n",
            "Words: 0.009*\"articl\" + 0.008*\"time\" + 0.008*\"like\" + 0.007*\"think\" + 0.006*\"know\" + 0.005*\"peopl\" + 0.005*\"good\" + 0.005*\"state\" + 0.005*\"univers\" + 0.005*\"say\"\n",
            "Topic: 3 \n",
            "Words: 0.009*\"file\" + 0.007*\"univers\" + 0.006*\"know\" + 0.006*\"nntp\" + 0.006*\"host\" + 0.006*\"like\" + 0.005*\"articl\" + 0.005*\"state\" + 0.005*\"card\" + 0.005*\"work\"\n",
            "Topic: 4 \n",
            "Words: 0.012*\"peopl\" + 0.007*\"say\" + 0.006*\"think\" + 0.006*\"armenian\" + 0.006*\"know\" + 0.006*\"time\" + 0.005*\"go\" + 0.005*\"like\" + 0.005*\"come\" + 0.004*\"articl\"\n",
            "Topic: 5 \n",
            "Words: 0.008*\"christian\" + 0.008*\"know\" + 0.007*\"say\" + 0.007*\"think\" + 0.007*\"believ\" + 0.006*\"jesu\" + 0.006*\"articl\" + 0.005*\"like\" + 0.005*\"univers\" + 0.005*\"peopl\"\n",
            "Topic: 6 \n",
            "Words: 0.007*\"space\" + 0.006*\"articl\" + 0.005*\"scsi\" + 0.005*\"like\" + 0.005*\"host\" + 0.005*\"program\" + 0.005*\"data\" + 0.005*\"chip\" + 0.005*\"need\" + 0.005*\"nntp\"\n",
            "Topic: 7 \n",
            "Words: 0.017*\"window\" + 0.008*\"problem\" + 0.007*\"drive\" + 0.006*\"know\" + 0.006*\"like\" + 0.005*\"work\" + 0.005*\"articl\" + 0.005*\"think\" + 0.004*\"file\" + 0.004*\"access\"\n",
            "Topic: 8 \n",
            "Words: 0.006*\"mail\" + 0.006*\"host\" + 0.006*\"univers\" + 0.006*\"articl\" + 0.006*\"avail\" + 0.006*\"program\" + 0.005*\"nntp\" + 0.005*\"inform\" + 0.005*\"window\" + 0.005*\"need\"\n",
            "Topic: 9 \n",
            "Words: 0.008*\"peopl\" + 0.006*\"articl\" + 0.005*\"armenian\" + 0.005*\"think\" + 0.005*\"time\" + 0.004*\"like\" + 0.004*\"want\" + 0.004*\"univers\" + 0.004*\"go\" + 0.004*\"know\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LMD9aWN6KQBE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "04167e2a-8cf5-4e6c-ed75-3aa1637057f8"
      },
      "source": [
        "#Running LDA using TFIDF\n",
        "lda_model_tfidf = gensim.models.LdaMulticore(corpus_tfidf, num_topics=10, id2word=dictionary, passes=2, workers=4)\n",
        "for idx, topic in lda_model_tfidf.print_topics(-1):\n",
        "    print('Topic: {} Word: {}'.format(idx, topic))"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Topic: 0 Word: 0.004*\"pitt\" + 0.004*\"gordon\" + 0.004*\"bank\" + 0.003*\"israel\" + 0.003*\"buffalo\" + 0.002*\"file\" + 0.002*\"articl\" + 0.002*\"univers\" + 0.002*\"window\" + 0.002*\"surrend\"\n",
            "Topic: 1 Word: 0.002*\"like\" + 0.002*\"know\" + 0.002*\"articl\" + 0.002*\"univers\" + 0.002*\"peopl\" + 0.002*\"say\" + 0.002*\"year\" + 0.002*\"time\" + 0.002*\"sandvik\" + 0.002*\"steve\"\n",
            "Topic: 2 Word: 0.003*\"virginia\" + 0.002*\"cramer\" + 0.002*\"univers\" + 0.002*\"problem\" + 0.002*\"window\" + 0.002*\"know\" + 0.002*\"mail\" + 0.002*\"nasa\" + 0.002*\"space\" + 0.002*\"optilink\"\n",
            "Topic: 3 Word: 0.002*\"chip\" + 0.002*\"window\" + 0.002*\"know\" + 0.002*\"encrypt\" + 0.002*\"thank\" + 0.002*\"driver\" + 0.002*\"clipper\" + 0.002*\"problem\" + 0.002*\"drive\" + 0.002*\"netcom\"\n",
            "Topic: 4 Word: 0.003*\"armenian\" + 0.003*\"govern\" + 0.003*\"peopl\" + 0.002*\"encrypt\" + 0.002*\"isra\" + 0.002*\"right\" + 0.002*\"clipper\" + 0.002*\"turkish\" + 0.002*\"chip\" + 0.002*\"stratu\"\n",
            "Topic: 5 Word: 0.002*\"cwru\" + 0.002*\"drive\" + 0.002*\"univers\" + 0.002*\"peopl\" + 0.002*\"printer\" + 0.002*\"know\" + 0.002*\"window\" + 0.002*\"think\" + 0.002*\"thank\" + 0.002*\"like\"\n",
            "Topic: 6 Word: 0.002*\"bike\" + 0.002*\"drive\" + 0.002*\"thank\" + 0.002*\"window\" + 0.002*\"univers\" + 0.002*\"like\" + 0.002*\"sale\" + 0.002*\"know\" + 0.002*\"good\" + 0.002*\"host\"\n",
            "Topic: 7 Word: 0.003*\"team\" + 0.003*\"game\" + 0.002*\"year\" + 0.002*\"think\" + 0.002*\"player\" + 0.002*\"peopl\" + 0.002*\"like\" + 0.002*\"articl\" + 0.002*\"play\" + 0.002*\"good\"\n",
            "Topic: 8 Word: 0.003*\"christian\" + 0.003*\"jesu\" + 0.002*\"church\" + 0.002*\"believ\" + 0.002*\"peopl\" + 0.002*\"think\" + 0.002*\"game\" + 0.002*\"know\" + 0.002*\"say\" + 0.002*\"univers\"\n",
            "Topic: 9 Word: 0.006*\"window\" + 0.004*\"file\" + 0.003*\"card\" + 0.003*\"color\" + 0.003*\"problem\" + 0.002*\"monitor\" + 0.002*\"drive\" + 0.002*\"program\" + 0.002*\"help\" + 0.002*\"thank\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NADiwj-IKkuc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 789
        },
        "outputId": "98c0e3b9-167d-48db-bd3e-25faff3bd374"
      },
      "source": [
        "processed_docs[0]"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['lerxst',\n",
              " 'thing',\n",
              " 'subject',\n",
              " 'nntp',\n",
              " 'post',\n",
              " 'host',\n",
              " 'organ',\n",
              " 'univers',\n",
              " 'maryland',\n",
              " 'colleg',\n",
              " 'park',\n",
              " 'line',\n",
              " 'wonder',\n",
              " 'enlighten',\n",
              " 'door',\n",
              " 'sport',\n",
              " 'look',\n",
              " 'late',\n",
              " 'earli',\n",
              " 'call',\n",
              " 'bricklin',\n",
              " 'door',\n",
              " 'small',\n",
              " 'addit',\n",
              " 'bumper',\n",
              " 'separ',\n",
              " 'rest',\n",
              " 'bodi',\n",
              " 'know',\n",
              " 'tellm',\n",
              " 'model',\n",
              " 'engin',\n",
              " 'spec',\n",
              " 'year',\n",
              " 'product',\n",
              " 'histori',\n",
              " 'info',\n",
              " 'funki',\n",
              " 'look',\n",
              " 'mail',\n",
              " 'thank',\n",
              " 'bring',\n",
              " 'neighborhood',\n",
              " 'lerxst']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4yDWkWOKpjl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "outputId": "d009e64e-f110-4c17-9e56-af409ba7c8af"
      },
      "source": [
        "for index, score in sorted(lda_model[bow_corpus[0]], key=lambda tup: -1*tup[1]):\n",
        "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model.print_topic(index, 10)))"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Score: 0.43487852811813354\t \n",
            "Topic: 0.007*\"space\" + 0.006*\"articl\" + 0.005*\"scsi\" + 0.005*\"like\" + 0.005*\"host\" + 0.005*\"program\" + 0.005*\"data\" + 0.005*\"chip\" + 0.005*\"need\" + 0.005*\"nntp\"\n",
            "\n",
            "Score: 0.27545031905174255\t \n",
            "Topic: 0.008*\"peopl\" + 0.006*\"articl\" + 0.005*\"armenian\" + 0.005*\"think\" + 0.005*\"time\" + 0.004*\"like\" + 0.004*\"want\" + 0.004*\"univers\" + 0.004*\"go\" + 0.004*\"know\"\n",
            "\n",
            "Score: 0.2702215015888214\t \n",
            "Topic: 0.008*\"univers\" + 0.007*\"articl\" + 0.007*\"year\" + 0.006*\"know\" + 0.006*\"host\" + 0.006*\"nntp\" + 0.005*\"team\" + 0.005*\"like\" + 0.004*\"think\" + 0.004*\"david\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DpyC-QQ9LJnA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "c23c0df8-837a-4736-c533-13b23c5bd90d"
      },
      "source": [
        "#Performance evaluation by classifying sample document using LDA TF-IDF model.\n",
        "for index, score in sorted(lda_model_tfidf[bow_corpus[0]], key=lambda tup: -1*tup[1]):\n",
        "    print(\"\\nScore: {}\\t \\nTopic: {}\".format(score, lda_model_tfidf.print_topic(index, 10)))"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Score: 0.6385207176208496\t \n",
            "Topic: 0.003*\"team\" + 0.003*\"game\" + 0.002*\"year\" + 0.002*\"think\" + 0.002*\"player\" + 0.002*\"peopl\" + 0.002*\"like\" + 0.002*\"articl\" + 0.002*\"play\" + 0.002*\"good\"\n",
            "\n",
            "Score: 0.3392506539821625\t \n",
            "Topic: 0.003*\"virginia\" + 0.002*\"cramer\" + 0.002*\"univers\" + 0.002*\"problem\" + 0.002*\"window\" + 0.002*\"know\" + 0.002*\"mail\" + 0.002*\"nasa\" + 0.002*\"space\" + 0.002*\"optilink\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoTkSvsELYuY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "68a1a3bb-b733-4c69-a399-1714acf325d3"
      },
      "source": [
        "#Testing model on unseen document\n",
        "unseen_document = 'How a Pentagon deal became an identity crisis for Google'\n",
        "bow_vector = dictionary.doc2bow(preprocess(unseen_document))\n",
        "for index, score in sorted(lda_model[bow_vector], key=lambda tup: -1*tup[1]):\n",
        "    print(\"Score: {}\\t Topic: {}\".format(score, lda_model.print_topic(index, 5)))"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score: 0.7749364972114563\t Topic: 0.012*\"peopl\" + 0.007*\"say\" + 0.006*\"think\" + 0.006*\"armenian\" + 0.006*\"know\"\n",
            "Score: 0.025016067549586296\t Topic: 0.009*\"file\" + 0.007*\"univers\" + 0.006*\"know\" + 0.006*\"nntp\" + 0.006*\"host\"\n",
            "Score: 0.025007765740156174\t Topic: 0.008*\"christian\" + 0.008*\"know\" + 0.007*\"say\" + 0.007*\"think\" + 0.007*\"believ\"\n",
            "Score: 0.02500748075544834\t Topic: 0.007*\"space\" + 0.006*\"articl\" + 0.005*\"scsi\" + 0.005*\"like\" + 0.005*\"host\"\n",
            "Score: 0.02500622719526291\t Topic: 0.008*\"univers\" + 0.007*\"articl\" + 0.007*\"year\" + 0.006*\"know\" + 0.006*\"host\"\n",
            "Score: 0.02500622719526291\t Topic: 0.009*\"articl\" + 0.008*\"time\" + 0.008*\"like\" + 0.007*\"think\" + 0.006*\"know\"\n",
            "Score: 0.025005655363202095\t Topic: 0.008*\"peopl\" + 0.006*\"articl\" + 0.005*\"armenian\" + 0.005*\"think\" + 0.005*\"time\"\n",
            "Score: 0.025005513802170753\t Topic: 0.006*\"mail\" + 0.006*\"host\" + 0.006*\"univers\" + 0.006*\"articl\" + 0.006*\"avail\"\n",
            "Score: 0.025004345923662186\t Topic: 0.011*\"articl\" + 0.009*\"think\" + 0.007*\"like\" + 0.006*\"game\" + 0.006*\"nntp\"\n",
            "Score: 0.02500426396727562\t Topic: 0.017*\"window\" + 0.008*\"problem\" + 0.007*\"drive\" + 0.006*\"know\" + 0.006*\"like\"\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}